{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport cv2\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom PIL import Image\nimport albumentations\nfrom albumentations import pytorch as AT\nfrom collections import OrderedDict\n\nimport os\nimport random\n\nimport pretrainedmodels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"../input/android-and-apple-smartphones-images/\"\nos.listdir(path)\n\ndef create_filelist(path):\n    filelist = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            filelist.append(os.path.join(dirname, filename))\n    return filelist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"android_train = path + \"Android_train/\"\nandroid_test = path + \"android_test/\"\niphone_train = path + \"Iphone_train/\"\napple_test = path + \"apple_test/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_filelist(apple_test)[0]\nprint(create_filelist(apple_test))\nplt.imshow(Image.open(create_filelist(apple_test)[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](num_classes=1000, pretrained=None)\nmodel.load_state_dict(torch.load(\"../input/se-resnext-pytorch-pretrained/se_resnext50_32x4d-a260b3a4.pth\"))\nmodel.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\nmodel.last_linear = nn.Linear(2048, 2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmartphoneDataset(Dataset):\n    \n    def __init__(self, filelist, transforms = None, mode=\"train\"):\n        self.filelist = filelist\n        self.transforms = transforms\n        self.mode = mode\n        if self.mode == \"train\":\n            self.label = 1  if \"Android_train\" in self.filelist[0] else 0\n            \n    def __len__(self):\n        return len(self.filelist)\n    \n    \n    def __getitem__(self, idx):\n        image_source = self.filelist[idx]\n        image = cv2.imread(image_source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transforms:\n            image = self.transforms(image=image)\n        if self.mode == \"train\":\n            return image[\"image\"], self.label\n        else:\n            return image[\"image\"], self.filelist[idx]\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 224\n\ndata_transforms = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n    albumentations.HorizontalFlip(),\n    #albumentations.RandomBrightness(),\n    albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n    #albumentations.HueSaturationValue(),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])\ndata_transforms_test = albumentations.Compose([\n    albumentations.Resize(img_size, img_size),\n    albumentations.Normalize(),\n    AT.ToTensor()\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"android_dataset = SmartphoneDataset(create_filelist(android_train), transforms = data_transforms)\niphone_dataset = SmartphoneDataset(create_filelist(iphone_train), transforms = data_transforms)\ntest_android_dataset = SmartphoneDataset(create_filelist(android_test), transforms = data_transforms_test, mode = \"test\")\ntest_apple_dataset = SmartphoneDataset(create_filelist(apple_test), transforms = data_transforms_test, mode = \"test\")\ntrain_dataset = ConcatDataset([android_dataset, iphone_dataset ])\ntest_dataset = ConcatDataset([test_android_dataset, test_apple_dataset ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nnum_workers = 0\n\n\n\ntrain_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True )\ntest_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples, labels = iter(train_loader).next()\nplt.figure(figsize=(16,24))\ngrid_imgs = torchvision.utils.make_grid(samples[:24])\nnp_grid_imgs = grid_imgs.numpy()\n# in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\nplt.imshow(np.transpose(np_grid_imgs, (1,2,0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = 0.001)\n#optimizer = optim.SGD(model.parameters(), lr = 0.001)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.15, patience = 2)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, criterion, optimizer, dataloader, num_epochs=15):\n    model.to(device)\n    train_loss = []\n    loss_min = np.Inf\n    patience = 5\n    stop = False\n    for epoch in range(1, num_epochs+1):\n        \n        running_loss = 0\n        correct_samples = 0\n        total_samples = 0\n        \n        for i, (image, label) in enumerate(dataloader):\n            image, label = image.to(device), label.to(device)\n            optimizer.zero_grad()\n            output = model(image)\n            \n            loss = criterion(output, label)\n            running_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            \n            _, indices = torch.max(output, 1)\n            correct_samples += torch.sum(indices == label)\n            total_samples += label.shape[0]\n            train_accuracy = float(correct_samples) / total_samples\n            \n            \n            \n            \n        scheduler.step()   \n        print(\"Epoch : {}/{}..\".format(epoch,num_epochs), \"Training Loss: {:.6f}\".format(running_loss/len(dataloader)), \"Acc: {:.6f}\".format(train_accuracy))\n        train_loss.append(running_loss)\n        \n        if  running_loss <= loss_min:\n            print('Loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            loss_min,\n            running_loss))\n            torch.save(model.state_dict(), 'model.pt')\n            loss_min = running_loss\n            p = 0\n        \n        if running_loss > loss_min:\n            p += 1\n            print(f'{p} epochs of increasing loss')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break        \n\n        if stop:\n            break\n    plt.plot(train_loss, label = \"Training Loss\")\n    plt.show()\n    return model\n                  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model = train_model(model, criterion, optimizer, train_loader, num_epochs = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model.eval()\nfn_list = []\npred_list = []\nfor image, filename in test_loader:\n    with torch.no_grad():\n        image = image.to(device)\n        output = trained_model(image)\n        pred = torch.argmax(output, dim=1)\n        fn_list += [n for n in filename]\n        pred_list += [p.item() for p in pred]\n\nsubmission = pd.DataFrame({\"id\":fn_list, \"label\":pred_list})\nsubmission.to_csv('preds.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(submission)):\n    if \"input/android-and-apple-smartphones-images/android_test/android_test/\" in submission[\"id\"][i]: \n        submission[\"fin\"][i] = 1\n    else:\n        submission[\"fin\"][i] = 0\n        \n        \ncorrect_samples = 0\ncorrect_samples += np.sum(submission[\"fin\"] == submission[\"label\"])\ntotal_samples = submission[\"label\"].shape[0]\nacc = float(correct_samples)/total_samples\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples, _ = iter(test_loader).next()\nsamples = samples.to(device)\nfig = plt.figure(figsize=(24, 16))\nfig.tight_layout()\noutput = model(samples[:24])\npred = torch.argmax(output, dim=1)\npred = [p.item() for p in pred]\nad = {0:'apple', 1:'android'}\nfor num, sample in enumerate(samples[:24]):\n    plt.subplot(4,6,num+1)\n    plt.title(ad[pred[num]])\n    plt.axis('off')\n    sample = sample.cpu().numpy()\n    plt.imshow(np.transpose(sample, (1,2,0)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}